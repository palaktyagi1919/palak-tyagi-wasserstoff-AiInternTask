# -*- coding: utf-8 -*-
"""file.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b6f_fKv8SEffaFOw8U2sEvoILelI6kWB
"""

from google.colab import drive
drive.mount('/content/drive/')

from PIL import Image
import torchvision.transforms as T

!pip install torch

image = Image.open("/content/drive/MyDrive/file 2.jpg")
transform = T.Compose([T.ToTensor()])
image_tensor = transform(image)

import torch

import torchvision.models as models
from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights

# Load a Mask R-CNN model with specific weights
model = models.detection.maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1)
model.eval()
# Check if GPU is available and use it if possible
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

with torch.no_grad():
    predictions = model([image_tensor])

import matplotlib.pyplot as plt
import numpy as np

def visualize_predictions(image, predictions, threshold=0.5):
    plt.imshow(image)
    for i, mask in enumerate(predictions[0]['masks']):
        if predictions[0]['scores'][i] > threshold:
            mask = mask[0].mul(255).byte().cpu().numpy()
            plt.imshow(mask, alpha=0.5)
    plt.show()

visualize_predictions(image, predictions)

"""visualize_predictions(image, predictions)"""







import os

def save_segmented_objects(image, predictions, output_dir="segmented_objects", threshold=0.5):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    for i, mask in enumerate(predictions[0]['masks']):
        if predictions[0]['scores'][i] > threshold:
            mask = mask[0].mul(255).byte().cpu().numpy()
            masked_image = np.array(image) * mask[:, :, np.newaxis]
            object_image = Image.fromarray(masked_image.astype(np.uint8))
            object_image.save(os.path.join(output_dir, f"object_{i}.png"))
            print(f"Saved object_{i}.png")

save_segmented_objects(image, predictions)

"""save_segmented_objects(image, predictions)"""

! pip install transformers

from transformers import CLIPProcessor, CLIPModel
import torch

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def describe_objects(output_dir="segmented_objects"):
    descriptions = {}
    for filename in os.listdir(output_dir):
        if filename.endswith(".png"):
            image_path = os.path.join(output_dir, filename)
            image = Image.open(image_path)
            inputs = processor(images=image, return_tensors="pt")
            outputs = model.get_text_features(**inputs)
            text = model.generate(inputs)
            descriptions[filename] = text

    return descriptions
    object_descriptions = describe_objects()

pip install pytesseract

import pytesseract

!sudo apt-get install tesseract-ocr

def extract_text_from_objects(output_dir="segmented_objects"):
    texts = {}
    for filename in os.listdir(output_dir):
        if filename.endswith(".png"):
            image_path = os.path.join(output_dir, filename)
            text = pytesseract.image_to_string(image_path)
            texts[filename] = text

    return texts
object_texts = extract_text_from_objects()

from transformers import pipeline

summarizer = pipeline("summarization")

def summarize_object_attributes(object_texts):
    summaries = {}
    for filename, text in object_texts.items():
        summary = summarizer(text, max_length=50, min_length=25, do_sample=False)
        summaries[filename] = summary[0]['summary_text']

    return summaries

object_summaries = summarize_object_attributes(object_texts)

import json
import os

def create_data_mapping(object_descriptions, object_texts, object_summaries, output_dir="segmented_objects"):
    data_mapping = {}

    try:
        for filename in os.listdir(output_dir):
            if filename.endswith(".png"):
                object_id = filename.split(".")[0]
                data_mapping[object_id] = {
                    "description": object_descriptions.get(filename, ""),
                    "text": object_texts.get(filename, ""),
                    "summary": object_summaries.get(filename, "")
                }

        with open("data_mapping.json", "w") as f:
            json.dump(data_mapping, f, indent=4)
    except FileNotFoundError as e:
        print(f"Error: Directory not found: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

# Example usage
object_descriptions = {"object1.png": "description1", "object2.png": "description2"}
object_texts = {"object1.png": "text1", "object2.png": "text2"}
object_summaries = {"object1.png": "summary1", "object2.png": "summary2"}

create_data_mapping(object_descriptions, object_texts, object_summaries)



import pandas as pd

def generate_final_output(image, data_mapping):
    fig, ax = plt.subplots(1, figsize=(12, 9))
    ax.imshow(image)
    table_data = []

    for object_id, data in data_mapping.items():
        table_data.append([object_id, data["description"], data["text"], data["summary"]])
        # Here you would add code to annotate the image with object IDs

    table = pd.DataFrame(table_data, columns=["Object ID", "Description", "Text", "Summary"])
    table.to_csv("object_data.csv", index=False)

    plt.show()

with open("data_mapping.json") as f:
    data_mapping = json.load(f)

generate_final_output(image, data_mapping)

!pip install streamlit

pip install opencv-python-headless pillow pandas

import streamlit as st
from PIL import Image
import pandas as pd
import cv2
import json
import warnings
warnings.filterwarnings("ignore",category=FutureWarning)

st.title("AI Pipeline for Image Segmentation and Object Analysis")

uploaded_file = st.file_uploader("World_map", type="jpg")
if uploaded_file is not None:
    image = Image.open(uploaded_file)
    st.image(image, caption='Uploaded Image', use_column_width=True)
    st.write("")
    st.write("Segmenting image...")

st.title("AI Pipeline for Image Segmentation and Object Analysis")

uploaded_file = st.file_uploader("/content/drive/MyDrive/file 2.jpg", type="jpg")
if uploaded_file is not None:
    image = Image.open(uploaded_file)
    st.image(image, caption='Uploaded Image', use_column_width=True)
    st.write("")
    st.write("Segmenting image...")

if uploaded_file is not None:
    # Dummy function for segmentation
    segmented_objects, master_id = segment_image(image)

    # Display segmented objects
    st.image(segmented_objects, caption='Segmented Objects', use_column_width=True)

if uploaded_file is not None:
    object_images = extract_objects(segmented_objects)

    for idx, obj in enumerate(object_images):
        st.image(obj, caption=f'Object {idx+1}', use_column_width=True)

if uploaded_file is not None:
    identified_objects = identify_objects(object_images)

    for idx, desc in enumerate(identified_objects):
        st.write(f"Object {idx+1}: {desc}")

if uploaded_file is not None:
    extracted_texts = extract_text(object_images)

    for idx, text in enumerate(extracted_texts):
        st.write(f"Object {idx+1} Text: {text}")

if uploaded_file is not None:
    summaries = summarize_attributes(identified_objects, extracted_texts)

    for idx, summary in enumerate(summaries):
        st.write(f"Object {idx+1} Summary: {summary}")

if uploaded_file is not None:
    # Create a DataFrame for the table
    data = {
        'Object ID': range(1, len(object_images) + 1),
        'Description': identified_objects,
        'Extracted Text': extracted_texts,
        'Summary': summaries
    }

    df = pd.DataFrame(data)
    st.write("Summary Table")
    st.dataframe(df)

    # Assuming you have a function to generate the final image with annotations
    annotated_image = generate_annotated_image(image, segmented_objects)
    st.image(annotated_image, caption='Annotated Image', use_column_width=True)

!streamlit run streamlit_app.py

streamlit run app.py

streamlit run image_segmentation_pipeline.py

st.title('AI Pipeline for Image Segmentation and Object Analysis')

uploaded_file = st.file_uploader("Choose an image...", type="jpg")
if uploaded_file is not None:
    image_path = os.path.join(INPUT_DIR, uploaded_file.name)
    with open(image_path, 'wb') as f:
        f.write(uploaded_file.getbuffer())
    st.image(image_path, caption='Uploaded Image.', use_column_width=True)

    if st.button('Process'):
        st.write("Processing...")
        predictions = segment_image(image_path)
        if predictions:
            save_segmented_objects(predictions, image_path)
            seg_image_path = visualize_segmentation(image_path, predictions)
            st.image(seg_image_path, caption='Segmented Image.', use_column_width=True)

            objects_data = {"image_id": uploaded_file.name, "objects": []}
            for idx, _ in enumerate(predictions[0]['masks']):
                object_path = os.path.join(SEGMENTED_DIR, f'object_{idx}.png')
                description = identify_objects(object_path)
                text = extract_text(object_path)
                summary = summarize_text(text)
                objects_data['objects'].append({"id": idx, "description": description, "text": text, "summary": summary})

            map_data(objects_data)
            generate_output_table(objects_data)
            st.write(objects_data)
            st.write(pd.DataFrame(objects_data['objects']))

if __name__ == "__main__":
    st.write("Run the Streamlit app using the command: streamlit run streamlit_app.py")

!streamlit run streamlit_app.py