# -*- coding: utf-8 -*-
"""Another copy of file.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eR1KWJFdUIW38iD6oJXsgYC1EFV_qFtE
"""

from google.colab import drive
drive.mount('/content/drive/')



!pip install torch torchvision

from PIL import Image
import torchvision.transforms as T

!pip install torch

image = Image.open("/content/drive/MyDrive/Trafic_Dhaka.jpg")
transform = T.Compose([T.ToTensor()])
image_tensor = transform(image)

import torch

import torchvision.models as models
from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights

# Load a Mask R-CNN model with specific weights
model = models.detection.maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1)
model.eval()
# Check if GPU is available and use it if possible
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

with torch.no_grad():
    predictions = model([image_tensor])

import matplotlib.pyplot as plt
import numpy as np

def visualize_predictions(image, predictions, threshold=0.5):
    plt.imshow(image)
    for i, mask in enumerate(predictions[0]['masks']):
        if predictions[0]['scores'][i] > threshold:
            mask = mask[0].mul(255).byte().cpu().numpy()
            plt.imshow(mask, alpha=0.5)
    plt.show()

visualize_predictions(image, predictions)

"""visualize_predictions(image, predictions)"""

import os

def save_segmented_objects(image, predictions, output_dir="segmented_objects", threshold=0.5):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    for i, mask in enumerate(predictions[0]['masks']):
        if predictions[0]['scores'][i] > threshold:
            mask = mask[0].mul(255).byte().cpu().numpy()
            masked_image = np.array(image) * mask[:, :, np.newaxis]
            object_image = Image.fromarray(masked_image.astype(np.uint8))
            object_image.save(os.path.join(output_dir, f"object_{i}.png"))
            print(f"Saved object_{i}.png")

save_segmented_objects(image, predictions)

"""save_segmented_objects(image, predictions)"""

! pip install transformers

from transformers import CLIPProcessor, CLIPModel
import torch

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def describe_objects(output_dir="segmented_objects"):
    descriptions = {}
    for filename in os.listdir(output_dir):
        if filename.endswith(".png"):
            image_path = os.path.join(output_dir, filename)
            image = Image.open(image_path)
            inputs = processor(images=image, return_tensors="pt")
            outputs = model.get_text_features(**inputs)
            text = model.generate(inputs)
            descriptions[filename] = text

    return descriptions
    object_descriptions = describe_objects()

pip install pytesseract

import pytesseract

!sudo apt-get install tesseract-ocr

def extract_text_from_objects(output_dir="segmented_objects"):
    texts = {}
    for filename in os.listdir(output_dir):
        if filename.endswith(".png"):
            image_path = os.path.join(output_dir, filename)
            text = pytesseract.image_to_string(image_path)
            texts[filename] = text

    return texts
object_texts = extract_text_from_objects()

from transformers import pipeline

# Specify the model name explicitly
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

def summarize_object_attributes(object_texts):
    summaries = {}
    for filename, text in object_texts.items():
        try:
            # Generate the summary
            summary = summarizer(text, max_length=50, min_length=25, do_sample=False)
            summaries[filename] = summary[0]['summary_text']
        except Exception as e:
            print(f"Error summarizing text for {filename}: {e}")
            summaries[filename] = "Error summarizing text"

    return summaries

# Example usage
object_texts = {
    "example_file.txt": "This is an example text that needs to be summarized."
}

object_summaries = summarize_object_attributes(object_texts)
print(object_summaries)

import json
import os

def create_data_mapping(object_descriptions, object_texts, object_summaries, output_dir="segmented_objects"):
    data_mapping = {}

    try:
        # Iterate over files in the specified directory
        for filename in os.listdir(output_dir):
            if filename.endswith(".png"):
                object_id = filename.split(".")[0]
                # Populate the data mapping dictionary
                data_mapping[object_id] = {
                    "description": object_descriptions.get(filename, ""),
                    "text": object_texts.get(filename, ""),
                    "summary": object_summaries.get(filename, "")
                }

        # Save the data mapping to a JSON file
        with open("data_mapping.json", "w") as f:
            json.dump(data_mapping, f, indent=4)
        print("Data mapping saved successfully.")

    except FileNotFoundError as e:
        print(f"Error: Directory not found: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

# Example usage
object_descriptions = {"object1.png": "description1", "object2.png": "description2"}
object_texts = {"object1.png": "text1", "object2.png": "text2"}
object_summaries = {"object1.png": "summary1", "object2.png": "summary2"}

create_data_mapping(object_descriptions, object_texts, object_summaries)



import warnings
warnings.filterwarnings('ignore')

import torch
import torchvision.models as models
import torchvision.transforms as T
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import os

# Load the Mask R-CNN model
model = models.detection.maskrcnn_resnet50_fpn(pretrained=True)
model.eval()

# Check if GPU is available and use it if possible
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Load and preprocess the image
def load_image(image_path):
    image = Image.open(image_path).convert("RGB")
    transform = T.Compose([T.ToTensor()])
    image_tensor = transform(image).unsqueeze(0).to(device)
    return image, image_tensor

def visualize_predictions(image, predictions, threshold=0.5):
    plt.figure(figsize=(12, 9))
    plt.imshow(image)
    ax = plt.gca()

    # Loop through predictions and draw masks
    for i, (mask, score) in enumerate(zip(predictions[0]['masks'], predictions[0]['scores'])):
        if score > threshold:
            mask = mask[0].mul(255).byte().cpu().numpy()
            plt.imshow(mask, cmap='jet', alpha=0.5)

    plt.axis('off')
    plt.show()

# Main function to run detection and visualization
def main(image_path):
    image, image_tensor = load_image(image_path)

    with torch.no_grad():
        predictions = model(image_tensor)

    visualize_predictions(image, predictions)

    # Optionally save results or additional processing
    output_dir = 'detection_outputs'
    os.makedirs(output_dir, exist_ok=True)
    output_image_path = os.path.join(output_dir, 'detected_image.jpg')
    image.save(output_image_path)
    print(f"Detection results saved to {output_image_path}")

# Example usage
image_path = "/content/drive/MyDrive/imagefile.png"  # Adjust to your file location
main(image_path)

from google.colab import drive
drive.mount('/content/drive/')

from google.colab import drive
drive.mount('/content/drive')

import torch
import torchvision.transforms as T
import torchvision.models as models
from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights
from PIL import Image
import matplotlib.pyplot as plt
import os

def main(image_path):
    # Check if file exists
    if not os.path.exists(image_path):
        print(f"File not found: {image_path}")
        return

    # Load image and apply transformations
    image = Image.open(image_path).convert("RGB")  # Convert image to RGB
    transform = T.Compose([T.ToTensor()])
    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension

    # Load model
    model = models.detection.maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1)
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Run inference
    with torch.no_grad():
        predictions = model(image_tensor.to(device))

    # Visualization function
    def visualize_predictions(image, predictions, threshold=0.5):
        plt.imshow(image)
        ax = plt.gca()
        for i, score in enumerate(predictions[0]['scores']):
            if score > threshold:
                bbox = predictions[0]['boxes'][i].cpu().numpy()
                label = int(predictions[0]['labels'][i].cpu().numpy())
                score = score.cpu().numpy()
                ax.add_patch(plt.Rectangle(
                    (bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1],
                    fill=False, edgecolor='red', linewidth=3))
                ax.text(bbox[0], bbox[1] - 10, f"Label: {label} Score: {score:.2f}",
                        color='red', fontsize=12, weight='bold')
        plt.show()

    # Visualize the results
    visualize_predictions(image, predictions)

# Ensure Google Drive is mounted
from google.colab import drive
drive.mount('/content/drive/')

# Specify the path to your image
image_path = "/content/drive/MyDrive/Trafic_Dhaka.jpg"  # Replace with your file's correct path

# Run the main function with the correct argument
main(image_path)

import torch
import torchvision.transforms as T
import torchvision.models as models
from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights
from PIL import Image
import matplotlib.pyplot as plt
import os

# COCO class labels
COCO_CLASSES = [
    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',
    'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',
    'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',
    'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',
    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon',
    'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut',
    'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',
    'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',
    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]

def main(image_path):
    # Check if file exists
    if not os.path.exists(image_path):
        print(f"File not found: {image_path}")
        return

    # Load image and apply transformations
    image = Image.open(image_path).convert("RGB")  # Convert image to RGB
    transform = T.Compose([T.ToTensor()])
    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension

    # Load model
    model = models.detection.maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1)
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Run inference
    with torch.no_grad():
        predictions = model(image_tensor.to(device))

    # Visualization function
    def visualize_predictions(image, predictions, threshold=0.3):  # Lowered threshold
        plt.imshow(image)
        ax = plt.gca()
        for i, score in enumerate(predictions[0]['scores']):
            if score > threshold:
                bbox = predictions[0]['boxes'][i].cpu().numpy()
                label = int(predictions[0]['labels'][i].cpu().numpy())
                score = score.cpu().numpy()
                class_name = COCO_CLASSES[label]
                ax.add_patch(plt.Rectangle(
                    (bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1],
                    fill=False, edgecolor='red', linewidth=3))
                ax.text(bbox[0], bbox[1] - 10, f"{class_name}: {score:.2f}",
                        color='red', fontsize=12, weight='bold')
        plt.axis('off')  # Hide axes
        plt.show()

    # Visualize the results
    visualize_predictions(image, predictions)

# Ensure Google Drive is mounted
from google.colab import drive
drive.mount('/content/drive/')

# Specify the path to your image
image_path= "/content/drive/MyDrive/Trafic_Dhaka.jpg" # Replace with your file's correct path

# Run the main function with the correct argument
main(image_path)

import torch
import cv2
import numpy as np
from google.colab.patches import cv2_imshow

# Load the YOLOv5 model from PyTorch Hub
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

# Load an image using OpenCV
image_path ="/content/drive/MyDrive/Trafic_Dhaka.jpg" # Replace with your image path
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Perform inference
results = model(image_rgb)

# Parse the results
detections = results.pandas().xyxy[0]  # Bounding boxes in Pandas DataFrame

# Draw bounding boxes and labels on the image
for index, row in detections.iterrows():
    x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])
    label = row['name']
    confidence = row['confidence']

    # Draw bounding box
    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)

    # Put label and confidence
    text = f'{label} {confidence:.2f}'
    cv2.putText(image, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

# Save or display the result
cv2.imwrite('output_image.jpg', image)  # Save the result
cv2_imshow(image)  # Display the result in Colab

import torch
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import cv2
import numpy as np

# Load CLIP model and processor
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Load a pre-trained object detection model (YOLOv5s in this case)
yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

# Load an image
image_path =  "/content/drive/MyDrive/Trafic_Dhaka.jpg" # Replace with your image path
image = cv2.imread(image_path)

# Check if the image was loaded correctly
if image is None:
    raise FileNotFoundError(f"Image file at path '{image_path}' not found or cannot be loaded.")

# Convert image to RGB and PIL format
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
image_pil = Image.fromarray(image_rgb)

# Perform object detection
results = yolo_model(image_rgb)
detections = results.pandas().xyxy[0]  # Bounding boxes in Pandas DataFrame

# Define the list of labels for classification
labels = ["person", "car", "bus", "dog", "cat"]  # Extend this list based on your needs

# Draw bounding boxes and labels on the image
for index, row in detections.iterrows():
    x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])
    cropped_img = image_pil.crop((x1, y1, x2, y2))

    # Preprocess the cropped image and labels
    inputs = clip_processor(images=cropped_img, text=labels, return_tensors="pt", padding=True)

    # Perform CLIP classification
    with torch.no_grad():
        # Forward pass through the model
        outputs = clip_model(**inputs)
        image_features = outputs.image_embeds
        text_features = outputs.text_embeds

        # Compute similarity
        image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        logits_per_image = torch.matmul(image_features, text_features.T) * 100.0

        # Softmax to get probabilities
        probs = logits_per_image.softmax(dim=1)

    # Get the label with the highest probability
    label_idx = torch.argmax(probs, dim=1).item()
    label = labels[label_idx]
    confidence = probs[0][label_idx].item()

    # Draw bounding box and label
    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)
    text = f'{label} {confidence:.2f}'
    cv2.putText(image, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

# Save or display the result
output_image_path = 'output_image.jpg'
cv2.imwrite(output_image_path, image)
print(f'Result saved to {output_image_path}')

from google.colab.patches import cv2_imshow
import cv2

# Load the saved image
output_image_path = 'output_image.jpg'
output_image = cv2.imread(output_image_path)

# Display the image in Colab
cv2_imshow(output_image)

